<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>CVPR 2023:Query-Dependent Video Representation for Moment Retrieval and Highlight Detection | 魔法使いの秘密基地</title><meta name="keywords" content="归纳总结"><meta name="author" content="adventurer-w"><meta name="copyright" content="adventurer-w"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Baseline (Moment-DETR)视频时刻检索（Video Moment Retrieval）任务在形式上是指，给定一个未剪辑的视频和一个自然的句子查询，该任务旨在识别一个特定视频段的开始和结束时间戳，该视频段包含语义上与给定句子查询相对应的感兴趣的活动。Highlight Detection任务是指对每个video clips去判别是否为Highlight。 论文的baseline是M">
<meta property="og:type" content="article">
<meta property="og:title" content="CVPR 2023:Query-Dependent Video Representation for Moment Retrieval and Highlight Detection">
<meta property="og:url" content="http://example.com/2023/05/27/QD-DETR.md/index.html">
<meta property="og:site_name" content="魔法使いの秘密基地">
<meta property="og:description" content="Baseline (Moment-DETR)视频时刻检索（Video Moment Retrieval）任务在形式上是指，给定一个未剪辑的视频和一个自然的句子查询，该任务旨在识别一个特定视频段的开始和结束时间戳，该视频段包含语义上与给定句子查询相对应的感兴趣的活动。Highlight Detection任务是指对每个video clips去判别是否为Highlight。 论文的baseline是M">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/412C1CBF-59A3-49FE-88AC-6157AA95C616_1_105_c.jpeg">
<meta property="article:published_time" content="2023-05-27T14:56:12.000Z">
<meta property="article:modified_time" content="2023-05-27T05:38:17.815Z">
<meta property="article:author" content="adventurer-w">
<meta property="article:tag" content="归纳总结">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/412C1CBF-59A3-49FE-88AC-6157AA95C616_1_105_c.jpeg"><link rel="shortcut icon" href="/favicon.png"><link rel="canonical" href="http://example.com/2023/05/27/QD-DETR.md/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'CVPR 2023:Query-Dependent Video Representation for Moment Retrieval and Highlight Detection',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-05-27 13:38:17'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/iconfont.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/72805526.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">81</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/bilibili/"><i class="fa-fw fas fa-heart"></i><span> 追番</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/412C1CBF-59A3-49FE-88AC-6157AA95C616_1_105_c.jpeg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">魔法使いの秘密基地</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/bilibili/"><i class="fa-fw fas fa-heart"></i><span> 追番</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">CVPR 2023:Query-Dependent Video Representation for Moment Retrieval and Highlight Detection</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-05-27T14:56:12.000Z" title="发表于 2023-05-27 22:56:12">2023-05-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-05-27T05:38:17.815Z" title="更新于 2023-05-27 13:38:17">2023-05-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="CVPR 2023:Query-Dependent Video Representation for Moment Retrieval and Highlight Detection"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Baseline-Moment-DETR"><a href="#Baseline-Moment-DETR" class="headerlink" title="Baseline (Moment-DETR)"></a><strong>Baseline (Moment-DETR)</strong></h1><p>视频时刻检索（Video Moment Retrieval）任务在形式上是指，给定一个未剪辑的视频和一个自然的句子查询，该任务旨在识别一个特定视频段的开始和结束时间戳，该视频段包含语义上与给定句子查询相对应的感兴趣的活动。Highlight Detection任务是指对每个video clips去判别是否为Highlight。</p>
<p>论文的baseline是Moment-DETR（<em>QVHIGHLIGHTS: Detecting Moments and Highlightsin Videos via Natural Language Queries</em>）。在过去，尽管Moment Retrieval和Highlight Detection这两项任务有许多共同的特征（例如都需要学习用户文本查询和视频片段之间的相似性），但它们通常被单独研究，主要是由于缺乏在单一数据集中支持这两项任务的注释。而这篇论文首次使模型能够同时完成Moment Retrieval（MR） 和Highlight Detection（HD），并提出了QVHighlights 数据集。</p>
<p>作者从用于目标检测的DETR中获得灵感设计了Moment-DETR，这是一个端到端的Transformer Encoder-Decoder架构，将时刻检索视为直接的集合预测问题。通过这种方法，作者有效地消除了在时刻检索方法中常见的任何手动设计的预处理（如提议生成）或后处理（如非最大抑制）步骤的需要。作者进一步在编码器输出上添加了一个显著性排名目标，用于HD。尽管Moment-DETR在其设计中没有编码任何人类的先验知识，但他们的实验表明，与高度工程化的架构相比，它仍然具有竞争力。</p>
<p><img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/image-20230527132846350.png" alt="image-20230527132846350"></p>
<p>Moment-DET 模型概述。使用Transformer Encoder-Decoder和三个预测头来预测saliency score，前景/背景分数和矩坐标。该图中没有显示视频和文本特征提取器。</p>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>现有的一些研究在处理多模态源（例如视频和音频）时，虽然提出了一些突破性的架构，但是忽视了文本查询的作用。为了验证这个观点，作者对Moment-DETR进行了研究，发现在估计每个视频片段的查询相关性得分时，基线模型经常忽视给定的文本查询。</p>
<p>如图12所示，当给出相关和非相关查询时，高亮度（显著性得分）的比较。作者发现，Moment-DETR的查询基本没有发挥作用，因此可能无法检测到负面查询和视频-查询的相关性；ground-truth（GT）时刻的剪辑中，无论对于正面还是负面查询，显著性得分都较低且相当。另一方面，作者的方法（QD-DETR）的查询依赖表示导致了与视频-查询相关性相对应的显著性得分，并精确地定位了时刻。</p>
<p>为了解决这个问题，他们提出了Query-Dependent DETR (QD-DETR)，这是一种能够产生查询依赖的视频表示的模型。主要目标是确保模型对每个片段的预测高度依赖于查询。首先，为了充分利用查询中的上下文信息，作者修改了Transformer Encoder，使其在最初的层中配备了交叉注意力层。通过将视频作为查询，文本作为交叉注意力层的键和值，他们的编码器强制文本查询参与到视频表示的提取中。然后，为了不仅将大量的文本信息注入到视频特征中，而且还能充分利用它，他们还利用了由混合原始对生成的负视频-查询对。具体来说，模型被训练来抑制这种负（无关）对的显著性得分。作者期望的是，文本查询在预测中的贡献度会增加，因为视频有时需要根据文本查询是否相关，产生高的显著性得分，有时则需要产生低的显著性得分。最后，为了应用动态标准来标记每个实例的高亮部分，作者部署了一个显著性token来代表整个视频，并将其作为输入自适应的显著性标准。通过结合所有的组件，他们的QD-DETR通过整合源和查询模态，产生了查询依赖的视频表示。这进一步允许在transformer encoder中使用位置查询。</p>
<p><img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/image-20230527133008665.png" alt="image-20230527133008665"></p>
<p>这篇论文的主要贡献有以下三点</p>
<ol>
<li><p>提出了一种新的检测变换器，名为Query-Dependent DETR，用于视频理解中的时刻检索和高亮检测任务。QD-DETR引入了一个交叉注意力层，以利用文本查询中的上下文信息，并将视频-查询对组合起来，生成查询依赖的表示。</p>
</li>
<li><p>为了保持查询依赖的视频表示的多样性，定义了一个输入自适应的显著性预测器。简单来说，显著性标记是一个随机初始化的可学习向量，当它被添加到编码的视频标记的序列中并通过变换器编码器进行投影时，它就变成了一个输入自适应的预测器。</p>
</li>
<li><p>通过交叉注意力编码器确保了查询的贡献，同时通过负对训练强制模型学习查询和视频之间的关系，防止在不考虑查询的情况下解决问题。</p>
</li>
</ol>
<h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><p>时刻检索（MR）和高亮检测（HD）都有一个共同的目标，那就是找到与文本查询相匹配的优选时刻。给定一个由L个片段组成的视频和一个由N个词组成的文本查询，我们分别用冻结的视频和文本编码器（如CLIP）提取出他们的表示，记为{v1, v2, …, vL}和{t1, t2, …, tN}。有了这些表示，主要的目标就是在视频中定位中心坐标m_c和宽度m_σ，并为每个片段排名高亮得分（显著性得分）{s1, s2, …, sL}。</p>
<p>对于MR，利用Transformer的一个直接的方法是将时刻的预测作为一组片段，或者根据片段的预测生成时刻。为了利用多模态信息，过去的方法要么简单地将各模态的特征连接起来，要么插入文本以形成向transformer decoder的时刻查询。然而作者认为，视频和文本查询之间的关系应该被仔细考虑，而不是简单的连接，因为MR/HD需要每个视频片段都与文本查询有条件地进行评估。</p>
<p>QD-DETR的整体架构图如图13。使用Moment-DETR作为baseline。给定一个视频和文本查询，我们首先从固定的网络中提取视频和文本特征。这些视频和文本特征被传送到交叉注意力变换器（Cross-Attentive Transformer Encoder）。这个过程确保了文本查询对视频标记的一致贡献，并与Learning from Negative Relationship一起，构建了查询依赖的视频表示。然后伴随着saliency token，视频标记被给予transformer encoder。在这个过程中，saliency token记被转化为自适应的显著性预测标准。然后，encoder的输出被处理，以计算HD和MR的损失：具体来说，encoder的输出标记直接投影到saliency score，并优化HD，同时也提供给带有可学习时刻查询的transformer decoder，以估计查询描述的时刻。最后，通过预测的和对应的GT时刻之间的差异，计算MR的损失。</p>
<p><img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/image-20230527133047954.png" alt="image-20230527133047954"></p>
<h2 id="Cross-Attentive-Transformer-Encoder"><a href="#Cross-Attentive-Transformer-Encoder" class="headerlink" title="Cross-Attentive Transformer Encoder"></a>Cross-Attentive Transformer Encoder</h2><p><strong>这一节的内容，对应的是图左侧的红色部分。</strong></p>
<p>Encoder对于MR/HD的主要目标是产生带有查询相关度信息的片段表示，因为这些特征直接用于检索匹配查询的时刻和预测片段的saliency score。然而，现有工作的编码过程可能不能确保每个片段都有查询条件。例如，Moment-DETR简单地将视频和查询连接起来作为自注意力层的输入，如果视频片段之间的高相似性压倒了文本查询的贡献，那么查询可能就没有太大的作用。</p>
<p>为了将文本上下文引入每个视频片段表示，作者在encoder的最初几层之间都加上了视频源和查询模态之间的交叉注意力层（Cross-Attentive Transformer Encoder）。具体来说，交叉注意力层的查询是通过将视频片段投影为<img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/clip_image002.png" alt="img">来准备的，而键和值是通过查询文本特征计算得到的，即<img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/clip_image004.png" alt="img"> 和 <img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/clip_image006.png" alt="img">。<img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/clip_image008.png" alt="img">, 和<img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/clip_image010.png" alt="img">是查询、键和值的投影层。交叉注意力层按照以下方式操作：</p>
<p><img src="file:////Users/a26012/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image002.png" alt="img"></p>
<p>其中d是投影键、值和查询的维度。由于softmax得分只在查询元素上分布，所以视频片段是以与文本相似度成比例的文本查询的加权和来表示的。注意力得分然后通过MLP投影并集成到原始视频表示中，作为典型的Transformer层。将查询依赖的视频标记，即交叉注意力层的输出。</p>
<h2 id="Learning-from-Negative-Relationship"><a href="#Learning-from-Negative-Relationship" class="headerlink" title="Learning from Negative Relationship"></a>Learning from Negative Relationship</h2><p><strong>这一节的内容，对应的是图右上角的蓝色部分。</strong></p>
<p>虽然交叉注意力层明确地融合了视频和查询特征，以便在架构上参与查询信息的中间视频片段表示，但作者认为，给定的视频-文本对缺乏多样性，无法学习一般的关系。例如，单个视频中的许多连续片段通常具有相似的外观，与特定查询的相似性可能是高度可区分的。</p>
<p>因此，模型不仅考虑了与查询相关的视频片段，也考虑了与查询无关的视频片段，这种方法被称为负面配对学习。在负面配对学习中，给定的训练视频-查询对被定义为正面配对，而从不同对中混合视频和查询构造的对被定义为负面配对。如图14所示，在训练过程中，正面配对的视频片段被训练以产生与查询相关性的分段saliency score，而无关的负面视频-查询对被强制具有最低的saliency score。</p>
<p><img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/image-20230527133258311.png" alt="image-20230527133258311"></p>
<h2 id="Input-Adaptive-Saliency-Predictor"><a href="#Input-Adaptive-Saliency-Predictor" class="headerlink" title="Input-Adaptive Saliency Predictor"></a><strong>Input-Adaptive Saliency Predictor</strong></h2><p><strong>这一节的内容，对应的是图上方的绿色部分。</strong></p>
<p>这一节的内容，对应的是图13上方的绿色部分。这个部分提出了一个输入自适应的显著性预测器，这种方法的优点是能够根据输入的视频-查询对的特性，动态地调整显著性预测的标准，从而更好地适应各种不同的视频和查询对，提高了显著性预测的准确性。</p>
<p>对于显著性预测器S(·)，一个简单的实现方式是堆叠一个或多个全连接层。然而这样忽视了视频和自然语言查询对的多样性，违背了提取查询依赖视频表示的关键思想。</p>
<p>Input-Adaptive Saliency Predictor的核心思想是使用一个可学习的向量，称为显著性标记，作为预测器。这个显著性标记在被添加到编码的视频标记序列并通过Transformer encoder投影后，能够根据输入的特性进行自我调整，从而成为一个自适应的预测器。</p>
<p>具体来说，首先将显著性标记与查询依赖的视频标记连接起来，然后将这些标记送入变换器编码器进行处理。在这个过程中，显著性标记会根据输入的上下文信息进行重新组织。接下来，显著性和视频标记会被分别通过一个全连接层进行投影，得到的结果就是显著性得分。计算公式如下：</p>
<p><img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/%E6%88%AA%E5%B1%8F2023-05-27%2013.34.21.png" alt="截屏2023-05-27 13.34.21"></p>
<h2 id="LOSS"><a href="#LOSS" class="headerlink" title="LOSS"></a>LOSS</h2><p>这篇文章中，作者使用了几个损失函数来优化他们的模型。下面我将简单解释每个损失函数的意义和计算方法：</p>
<p><strong>Moment Retrieval Loss**</strong>：**这个损失函数用于衡量真实时刻和预测时刻之间的差异。它由L1损失、一种改进的IoU损失（LgIoU）以及交叉熵损失组成。L1损失和LgIoU损失用于定位视频中的时刻，而交叉熵损失用于将预测的时刻分类为前景或背景。计算公式如下：</p>
<p><img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/clip_image001.png" alt="img"></p>
<p><strong>Margin Ranking Loss**</strong>：**这个损失函数用于确保高排名的片段的显著性得分高于低排名的片段和负片段。</p>
<p><img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/clip_image002.png" alt="img"></p>
<p><strong>Rank-Aware Contrastive Loss**</strong>：**这个损失函数用于学习精确的显著性等级。它通过对比损失来学习每个片段的显著性得分。</p>
<p><img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/clip_image003.png" alt="img"></p>
<p><strong>Highlight Detection Loss**</strong>：**这个损失函数是Margin Ranking Loss和Rank-Aware Contrastive Loss的组合，用于估计显著性得分。</p>
<p><img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/image-20230527133511111.png" alt="image-20230527133511111"></p>
<p><strong>Total Loss**</strong>：**</p>
<p><img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/clip_image005.png" alt="img"></p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>作者使用了四个数据集：QVHighlights、Charades-STA、TVSum和ActivityNet Captions。这些数据集包含了各种类型的视频，如新闻、纪录片、vlog等。作者使用了与基线相同的评估指标。使用了IoU阈值为0.5和0.7的recall@1，以及不同阈值下的mAP。使用了mAP和HIT@1来评估高亮检测。HIT@1是通过最高得分剪辑的命中率来计算的。</p>
<p>在联合学习和预测MR/HD的任务中，QD-DETR在所有评估指标上都超过了最先进的方法。在使用视频源的方法中，QD-DETR在更严格的指标下显示出显著的增长；在高IOU下，它以大幅度的优势超过了之前的最先进的方法。在使用视频和音频源的QD-DETR中，与使用多模态源数据的最先进方法相比，MR的平均指标提高了11.84%。这些结果验证了在文本查询中强调源（仅视频或视频+音频）描述性上下文的重要性。</p>
<p><img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/image-20230527133526855.png" alt="image-20230527133526855"></p>
<h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a><strong>Ablation Study</strong></h2><p>在消融研究中，作者对每个组件的有效性进行了深入的研究。他们使用了交叉注意力变换器编码器（CATE）和动态锚点时刻（DAM）进行了比较。他们发现，CATE和DAM对于模型的性能提升起到了关键作用。此外，他们还发现，使用更多的自我注意力层并不能带来更好的性能，这证明了CATE的改进主要来自于强调文本查询的作用，而不仅仅是增加了更多的层。</p>
<p><img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/image-20230527133545761.png" alt="image-20230527133545761"></p>
<p><img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/image-20230527133549700.png" alt="image-20230527133549700"></p>
<p><img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/image-20230527133552766.png" alt="image-20230527133552766"></p>
<p>在定性结果部分，作者研究了查询依赖的视频表示如何敏感地反应文本查询上下文的变化。他们发现，查询与视频剪辑的相关性越高，查询的显著性得分就越高。例如，与视频实例完全无关的负面查询具有最低的得分，而半正面查询的得分位于正面和负面之间。此外，他们发现QD-DETR有时会提供比给定的真实时刻更精确的时刻预测。他们认为，对于正面查询，在非相关剪辑中稍微高一些的显著性得分是由于自我注意力层中的信息混合。</p>
<p><img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/image-20230527133610854.png" alt="image-20230527133610854"></p>
<h1 id="Limitation"><a href="#Limitation" class="headerlink" title="Limitation"></a>Limitation</h1><p>尽管文章的目标是强调文本查询在检索相关时刻和估计其与给定文本查询的一致性等级中的作用，但是，如果给定的查询不保持有意义的上下文，例如，提供了噪声文本查询（不匹配或无关的真实文本），那么训练可能不会那样有效。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">adventurer-w</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2023/05/27/QD-DETR.md/">http://example.com/2023/05/27/QD-DETR.md/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">魔法使いの秘密基地</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%BD%92%E7%BA%B3%E6%80%BB%E7%BB%93/">归纳总结</a></div><div class="post_share"><div class="social-share" data-image="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/412C1CBF-59A3-49FE-88AC-6157AA95C616_1_105_c.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/07/13/SRCL/"><img class="prev-cover" src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/A9CD8212-2784-45D6-9D4E-D39E860755A9_1_105_c.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">ACL2023:Vision Language Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation</div></div></a></div><div class="next-post pull-right"><a href="/2023/05/27/VDI/"><img class="next-cover" src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/2BFB88C0-7BD6-4523-8387-8CAFF7EACECA_1_105_c.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">CVPR 2023-Towards Generalisable Video Moment Retrieval:Visual-Dynamic Injection to Image-Text Pre-Training</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/01/09/6DPose-1/" title="6D姿态估计-笔记1"><img class="cover" src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/A3E5572F-5538-46C2-B4FD-ECC60C0B90E5_1_105_c.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-09</div><div class="title">6D姿态估计-笔记1</div></div></a></div><div><a href="/2022/09/29/Transformer1/" title="Transformer笔记1 —— BERT"><img class="cover" src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/9F442EA0-F893-45C6-8967-10A3799F6DFC.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-29</div><div class="title">Transformer笔记1 —— BERT</div></div></a></div><div><a href="/2022/09/29/Transformer2/" title="Transformer笔记2 —— VIT"><img class="cover" src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/703C5B20-1C26-4B29-B378-B5B9FD03B485_1_105_c.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-29</div><div class="title">Transformer笔记2 —— VIT</div></div></a></div><div><a href="/2023/02/01/SAR%20Change%20Detection/" title="SAR图像变化检测深度学习研究方法"><img class="cover" src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/A3E5572F-5538-46C2-B4FD-ECC60C0B90E5_1_105_c.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-01</div><div class="title">SAR图像变化检测深度学习研究方法</div></div></a></div><div><a href="/2022/09/30/Transformer3/" title="Transformer笔记3——SwinTransformer"><img class="cover" src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/761D60E7-9EB9-4498-9053-86BF95761992_1_102_o.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-30</div><div class="title">Transformer笔记3——SwinTransformer</div></div></a></div><div><a href="/2022/11/23/detection/" title="目标检测相关摘录"><img class="cover" src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/628FBB49-B697-4165-8B49-A59144FB233F_1_102_o.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-23</div><div class="title">目标检测相关摘录</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/72805526.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">adventurer-w</div><div class="author-info__description">励志好好学习魔法ヾ(=･ω･=)o</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">81</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/adventurer-w"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://space.bilibili.com/91422238" target="_blank" title="BiliBili"><i class="iconfont icon-bilibili-line"></i></a><a class="social-icon" href="https://space.bilibili.com/91422238" target="_blank" title="BiliBili"><i class="iconfont icon-weibo"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Baseline-Moment-DETR"><span class="toc-number">1.</span> <span class="toc-text">Baseline (Moment-DETR)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Motivation"><span class="toc-number">2.</span> <span class="toc-text">Motivation</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Methodology"><span class="toc-number">3.</span> <span class="toc-text">Methodology</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Cross-Attentive-Transformer-Encoder"><span class="toc-number">3.1.</span> <span class="toc-text">Cross-Attentive Transformer Encoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Learning-from-Negative-Relationship"><span class="toc-number">3.2.</span> <span class="toc-text">Learning from Negative Relationship</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Input-Adaptive-Saliency-Predictor"><span class="toc-number">3.3.</span> <span class="toc-text">Input-Adaptive Saliency Predictor</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LOSS"><span class="toc-number">3.4.</span> <span class="toc-text">LOSS</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Experiments"><span class="toc-number">4.</span> <span class="toc-text">Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Ablation-Study"><span class="toc-number">4.1.</span> <span class="toc-text">Ablation Study</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Limitation"><span class="toc-number">5.</span> <span class="toc-text">Limitation</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/07/13/SRCL/" title="ACL2023:Vision Language Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation"><img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/A9CD8212-2784-45D6-9D4E-D39E860755A9_1_105_c.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ACL2023:Vision Language Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation"/></a><div class="content"><a class="title" href="/2023/07/13/SRCL/" title="ACL2023:Vision Language Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation">ACL2023:Vision Language Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation</a><time datetime="2023-07-13T04:56:11.000Z" title="发表于 2023-07-13 12:56:11">2023-07-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/27/QD-DETR.md/" title="CVPR 2023:Query-Dependent Video Representation for Moment Retrieval and Highlight Detection"><img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/412C1CBF-59A3-49FE-88AC-6157AA95C616_1_105_c.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CVPR 2023:Query-Dependent Video Representation for Moment Retrieval and Highlight Detection"/></a><div class="content"><a class="title" href="/2023/05/27/QD-DETR.md/" title="CVPR 2023:Query-Dependent Video Representation for Moment Retrieval and Highlight Detection">CVPR 2023:Query-Dependent Video Representation for Moment Retrieval and Highlight Detection</a><time datetime="2023-05-27T14:56:12.000Z" title="发表于 2023-05-27 22:56:12">2023-05-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/27/VDI/" title="CVPR 2023-Towards Generalisable Video Moment Retrieval:Visual-Dynamic Injection to Image-Text Pre-Training"><img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/2BFB88C0-7BD6-4523-8387-8CAFF7EACECA_1_105_c.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CVPR 2023-Towards Generalisable Video Moment Retrieval:Visual-Dynamic Injection to Image-Text Pre-Training"/></a><div class="content"><a class="title" href="/2023/05/27/VDI/" title="CVPR 2023-Towards Generalisable Video Moment Retrieval:Visual-Dynamic Injection to Image-Text Pre-Training">CVPR 2023-Towards Generalisable Video Moment Retrieval:Visual-Dynamic Injection to Image-Text Pre-Training</a><time datetime="2023-05-27T14:56:11.000Z" title="发表于 2023-05-27 22:56:11">2023-05-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/12/diffusion/" title="万字解读~~~扩散模型和跨模态生成(内含数学推导)"><img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/%E6%88%AA%E5%B1%8F2023-05-26%2015.02.42.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="万字解读~~~扩散模型和跨模态生成(内含数学推导)"/></a><div class="content"><a class="title" href="/2023/05/12/diffusion/" title="万字解读~~~扩散模型和跨模态生成(内含数学推导)">万字解读~~~扩散模型和跨模态生成(内含数学推导)</a><time datetime="2023-05-12T14:56:11.000Z" title="发表于 2023-05-12 22:56:11">2023-05-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/10/CLIP/" title="万字解读~~~大规模预训练模型在视觉理解中的应用——以CLIP为例"><img src="https://tuchuang-1308516817.cos.ap-nanjing.myqcloud.com/img/A9AF49B6-C7E1-43E6-9E62-7394CA513346.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="万字解读~~~大规模预训练模型在视觉理解中的应用——以CLIP为例"/></a><div class="content"><a class="title" href="/2023/05/10/CLIP/" title="万字解读~~~大规模预训练模型在视觉理解中的应用——以CLIP为例">万字解读~~~大规模预训练模型在视觉理解中的应用——以CLIP为例</a><time datetime="2023-05-10T14:56:11.000Z" title="发表于 2023-05-10 22:56:11">2023-05-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By adventurer-w</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'WmXbg5ij5vMxwX438aE8YqmW-gzGzoHsz',
      appKey: '1csr0uQv2Nu1k8EmKultS9gQ',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'zh-CN',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
      requiredFields: ["nick,mail"],
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>